{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds\n",
    "from surprise import Reader\n",
    "from surprise import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Recommender System\n",
    "In this lab session, we will work with the training set created last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307a43aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('All_Beauty_5.json.gz')\n",
    "\n",
    "df = df.sort_values(by=['reviewerID', 'asin', 'unixReviewTime'])\n",
    "cleaned_dataset = df.dropna(subset=['overall']).drop_duplicates(subset=['reviewerID', 'asin'], keep = 'last').reset_index(drop=True)\n",
    "# print(len(cleaned_dataset))\n",
    "# cleaned_dataset.head()\n",
    "cleaned_dataset = cleaned_dataset.sort_values(by=['reviewerID', 'unixReviewTime']).reset_index(drop=True)\n",
    "# extracting the latest (in time) positively rated item (rating  â‰¥4 ) by each user. \n",
    "test_data_pre = cleaned_dataset[cleaned_dataset.overall >= 4.0].drop_duplicates(subset=['reviewerID'], keep='last')\n",
    "# generate training data\n",
    "training_data = cleaned_dataset.drop(test_data_pre.index)\n",
    "\n",
    "# Remove users that do not appear in the training set.\n",
    "user_in_training = test_data_pre['reviewerID'].isin(training_data['reviewerID'])\n",
    "test_data = test_data_pre[user_in_training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67289199",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.DataFrame(columns = training_data['asin'].drop_duplicates(), index = training_data['reviewerID'].drop_duplicates())\n",
    "for row_i in range(len(training_data)):\n",
    "    reviewerID = list(training_data.reviewerID)[row_i]\n",
    "    asin = list(training_data.asin)[row_i]\n",
    "    rate = list(training_data.overall)[row_i]\n",
    "    matrix.loc[reviewerID, asin] = rate\n",
    "matrix = matrix.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "In this exercise, we are going to predict the rating of a single user-item pair using a neighborhood-based method.\n",
    "### 1.1\n",
    "- Represent the ratings from the training set in a user-item matrix where the rows represent users and the columns represent items.\n",
    "- Fill unobserved ratings with $0$.\n",
    "\n",
    "Compute the cosine similarities between the user with 'reviewerID'='A25C2M3QF9G7OQ' and all users that have rated the item with 'asin'='B00EYZY6LQ'.<br>\n",
    "What are the similarities and what are the ratings given by these users on item 'B00EYZY6LQ'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = cosine_similarity(np.array(matrix.loc[matrix.index == 'A25C2M3QF9G7OQ']),matrix[matrix.loc[:,'B00EYZY6LQ']>0])\n",
    "\n",
    "result = pd.DataFrame(matrix[matrix.loc[:,'B00EYZY6LQ']>0].loc[:,'B00EYZY6LQ'])\n",
    "result.insert(result.shape[1],'cos_sim',np.array(cosine_similarities).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "Predict the rating for user 'A25C2M3QF9G7OQ' on item 'B00EYZY6LQ' based on the ratings from the $3$ most similar users, using a weighted (by similarity) average. What is the prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5feb6240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                B00EYZY6LQ   cos_sim\n",
      "reviewerID                          \n",
      "A2ZY49IDE6TY5I         4.0  0.682835\n",
      "A2LW5AL0KQ9P1M         4.0  0.275810\n",
      "A1R1BFJCMWX0Y3         3.0  0.245145\n"
     ]
    }
   ],
   "source": [
    "result_sort = result.sort_values(by='cos_sim', ascending=False).head(3)\n",
    "print(result_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction is 3.79635550\n"
     ]
    }
   ],
   "source": [
    "result_sort = result.sort_values(by='cos_sim', ascending=False).head(3)\n",
    "prediction = 0\n",
    "for index in result_sort.index:\n",
    "    prediction += result_sort.loc[index][0]*result_sort.loc[index][1]\n",
    "prediction /= result_sort.loc[:,'cos_sim'].sum()\n",
    "print('prediction is {:.8f}'.format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "In this exercise, we are going to predict the rating of the same user-item pair as in exercise 1, now using a latent factor method.\n",
    "### 2.1\n",
    "- Represent the ratings from the training set in a user-item matrix where the rows represent users and the columns represent items.\n",
    "- Subtract the row mean (i.e. mean rating per user) from each non-missing element in the matrix.\n",
    "- Replace missing values with $0$.\n",
    "\n",
    "Factorize the user-item matrix by performing Singular Value Decomposition (SVD) of rank $5$ using eigendecomposition. What is ther user factors of user 'A25C2M3QF9G7OQ' and the item factors of item 'B00EYZY6LQ'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9274a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" user_item_pre = training_data.pivot('reviewerID', 'asin', 'overall')\\nuser_item_mean = user_item_pre.mean(axis = 1)\\nuser_item_sub = user_item_pre.sub(user_item_mean, axis = 0)\\nuser_item_sub = user_item_sub.fillna(0)\\nA25C2M3QF9G7OQ_mean = user_item_mean.loc['A25C2M3QF9G7OQ']\\nprint(A25C2M3QF9G7OQ_mean) \""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = pd.DataFrame(columns = training_data['asin'].drop_duplicates(), index = training_data['reviewerID'].drop_duplicates())\n",
    "for row_i in range(len(training_data)):\n",
    "    reviewerID = list(training_data.reviewerID)[row_i]\n",
    "    asin = list(training_data.asin)[row_i]\n",
    "    rate = list(training_data.overall)[row_i]\n",
    "    matrix.loc[reviewerID, asin] = rate\n",
    "matrix_0 = matrix.fillna(0)\n",
    "matrix = matrix.fillna(0)\n",
    "for index in matrix.index:\n",
    "    mean_ = np.mean([i for i  in matrix.loc[index] if i != 0])\n",
    "    if index=='A25C2M3QF9G7OQ':\n",
    "        A25C2M3QF9G7OQ_mean = mean_\n",
    "    for item in range(len(matrix.loc[index])):\n",
    "        if matrix.loc[index][item] != 0:\n",
    "            matrix.loc[index][item] -= mean_\n",
    "\n",
    "\"\"\" user_item_pre = training_data.pivot('reviewerID', 'asin', 'overall')\n",
    "user_item_mean = user_item_pre.mean(axis = 1)\n",
    "user_item_sub = user_item_pre.sub(user_item_mean, axis = 0)\n",
    "user_item_sub = user_item_sub.fillna(0)\n",
    "A25C2M3QF9G7OQ_mean = user_item_mean.loc['A25C2M3QF9G7OQ']\n",
    "print(A25C2M3QF9G7OQ_mean) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, sigma, P = svds(matrix, k=5)\n",
    "U = np.dot(Q, np.diag(sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51cb39fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -0.553446\n",
      "1   -0.421214\n",
      "2   -0.063396\n",
      "3    0.656496\n",
      "4    0.251410\n",
      "Name: A25C2M3QF9G7OQ, dtype: float64\n",
      "0    0.054085\n",
      "1   -0.009215\n",
      "2    0.040723\n",
      "3    0.042454\n",
      "4    0.152673\n",
      "Name: B00EYZY6LQ, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "user_factors = pd.DataFrame(data = U, index = matrix.index)\n",
    "print(user_factors.loc['A25C2M3QF9G7OQ'])\n",
    "item_factors = pd.DataFrame(data = P, columns = matrix.columns)\n",
    "print(item_factors.loc[:,'B00EYZY6LQ'])\n",
    "u_A25C2M3QF9G7OQ = np.array(user_factors.loc['A25C2M3QF9G7OQ'])\n",
    "i_B00EYZY6LQ = np.array(item_factors.loc[:,'B00EYZY6LQ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "Predict the rating for user 'A25C2M3QF9G7OQ' on item 'B00EYZY6LQ' by taking the dot product between the user factors and item factors and adding back the mean rating of this user. What is the prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.437621\n"
     ]
    }
   ],
   "source": [
    "print(\"{:.6f}\".format(A25C2M3QF9G7OQ_mean+np.dot(u_A25C2M3QF9G7OQ,i_B00EYZY6LQ)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "For the rest of the exercises, you can use the python library Scikit-Surprise. Please find the documentation here: https://surprise.readthedocs.io/en/stable/getting_started.html. <br>\n",
    "You can convert the training set to the format required in Scikit-Surprise as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "training = Dataset.load_from_df(training_data[['reviewerID', 'asin', 'overall']], reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "### 3.1\n",
    "Define a user-based neighborhood model that takes into account the mean rating of each user.<br>\n",
    "Use cosine as similarity measure and try to vary the (maximum) number of neighbors to take into account when predicting ratings. Keep Scikit-Surprise's default setting for all other parameters. <br>\n",
    "Is it better to use $1$ or $10$ neighbors? You should determine this based on the Root Mean Square Error (RMSE) over 3-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.4205100191988406\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.43072198050222493\n"
     ]
    }
   ],
   "source": [
    "from surprise import KNNWithMeans\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import GridSearchCV\n",
    "param_grid = {'k': [1, 10],\n",
    "'sim_options' : {'name': ['cosine'],\n",
    "               'user_based': [True]  # compute  similarities between items\n",
    "               }\n",
    "               }\n",
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': True  # compute  similarities between items\n",
    "               }\n",
    "\n",
    "\n",
    "algo_1 = KNNWithMeans(k = 1 , min_k = 1, sim_options = sim_options)\n",
    "\n",
    "print(cross_validate(algo_1, training, measures=['RMSE'], cv=3, verbose=False)['test_rmse'].mean())\n",
    "\n",
    "algo_10 = KNNWithMeans(k = 10 , min_k = 1, sim_options = sim_options)\n",
    "\n",
    "print(cross_validate(algo_10, training, measures=['RMSE'], cv=3, verbose=False)['test_rmse'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64ca0b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.37609365440256365\n",
      "{'k': 10, 'sim_options': {'name': 'cosine', 'user_based': True}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gs = GridSearchCV(KNNWithMeans, param_grid, measures=['rmse'], cv=3)\n",
    "gs.fit(training)\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2\n",
    "Fit the neigborhood-based model defined in exercise 3.1 on the full training set with cosine as similarity measure and either $1$ or $10$ neighbors based on what you found to be better in exercise 3.1. Keep Scikit-Surprise's default setting for all other parameters, but set the random state to $0$ for comparable results. <br>\n",
    "Use the model to predict the unobserved ratings for the users in the training set. How many predictions are there and what is the average of all the predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "54746\n",
      "4.628144189949582\n"
     ]
    }
   ],
   "source": [
    "trainset = training.build_full_trainset()\n",
    "testset = trainset.build_anti_testset()\n",
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': True  # compute  similarities between items\n",
    "               }\n",
    "\n",
    "\n",
    "algo_10 = KNNWithMeans(k = 10 , min_k = 1, sim_options = sim_options)\n",
    "predictions = [i.est for i in algo_10.fit(trainset).test(testset)]\n",
    "\"\"\" for user in training_data.reviewerID:\n",
    "    for item in training_data.asin:\n",
    "        if matrix_0.loc[user, item] == 0:\n",
    "            predictions.append(algo_10.predict(uid=user,iid=item)) \"\"\"\n",
    "\n",
    "print(len(predictions))\n",
    "print(np.mean(predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "### 4.1\n",
    "Define an SVD model with user and item biases that uses Stochastic Gradient Descend (SGD) to estimate the low-rank matrix based on only observed ratings. <br>\n",
    "Set the number of latent factors to $30$ and try to iterate the SGD procedure for different number of epochs. Keep Scikit-Surprise's default setting for all other parameters. <br>\n",
    "Is it better to run for $100$ or $500$ epochs? You should determine this based on the RMSE over 3-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36163921298099866\n",
      "{'n_epochs': 500, 'n_factors': 30}\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise.model_selection import GridSearchCV\n",
    "param_grid = {'n_epochs': [100, 500],'n_factors':[30]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "gs.fit(training)\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2\n",
    "Fit the latent factor model defined in exercise 4.1 on the full training set with $30$ latent factors and run for either $100$ or $500$ epochs based on what you found to be better in exercise 4.1. Keep Scikit-Surprise's default setting for all other parameters, but set the random state to $0$ for comparable results.<br>\n",
    "Use the model to predict the unobserved ratings for the users in the training set. How many predictions are there and what is the average of all the predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54746\n",
      "4.403720461682863\n"
     ]
    }
   ],
   "source": [
    "algo = SVD(n_epochs=500, n_factors=30, random_state=0)\n",
    "predictions = [i.est for i in algo.fit(trainset).test(testset)]\n",
    "print(len(predictions))\n",
    "print(np.mean(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
