{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6011ca",
   "metadata": {},
   "source": [
    "# Content-based recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a455b8a",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Based on the TF-IDF vectors obtained in the Exercise 2 from Session 4, represent each user in the same vector space. Amongst other feasible solutions, you can represent a user (user profile) by computing the weighted mean of the items vectors. Compute the cosine similarity for user 'A39WWMBA0299ZF' and all products in the training set not rated by the user. What are the top-5 recommended items for user 'A39WWMBA0299ZF'? Print out the top-5 items and their similarity score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be516e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lwk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/lwk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "403ff409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load TRAIN and TEST sets \n",
    "test_data = pickle.load( open( \"test.pkl\", \"rb\" ) )\n",
    "train_data = pickle.load( open( \"training.pkl\", \"rb\" ) )\n",
    "\n",
    "# Load the METADATA (ITEMS)\n",
    "df = pd.read_json('meta_All_Beauty.json', lines=True)\n",
    "\n",
    "# Discard duplicates\n",
    "df = df.sort_values(by=['asin'])\n",
    "clean_dataset_item = df.drop_duplicates(subset=['asin'], keep = 'last').reset_index(drop=True)\n",
    "\n",
    "# Discard items that weren't rated by our subset of users\n",
    "item_in_subset = list(test_data.loc[:,'asin'])+list(train_data.loc[:,'asin'])\n",
    "# print(list(item_in_subset))\n",
    "clean_dataset_item = clean_dataset_item[clean_dataset_item['asin'].isin(item_in_subset)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e080b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1039\n",
      "1002\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "len_words = 0\n",
    "len_filter_words = 0\n",
    "title_list = []\n",
    "temp_list = []\n",
    "for title in clean_dataset_item['title']:\n",
    "    # print(title)\n",
    "    word_list = [word for word in word_tokenize(title)]\n",
    "    temp_list += word_list\n",
    "    # temp_list.append(word_list)\n",
    "len_words = len(temp_list)\n",
    "# temp_list_ = []\n",
    "# print(temp_list.count('3.5'))\n",
    "for title in clean_dataset_item['title']:\n",
    "    filter_list = [porter_stemmer.stem(word) for word in word_tokenize(title) if word not in stopwords.words(\"english\")]\n",
    "    len_filter_words += len(filter_list)\n",
    "    # temp_list_ += filter_list\n",
    "    title_list.append(TreebankWordDetokenizer().detokenize(filter_list))\n",
    "    # title_list.append(\" \".join(filter))\n",
    "print(len_words)\n",
    "print(len_filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 449)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# <YOUR CODE HERE>\n",
    "X = tfidf_vectorizer.fit_transform(title_list)\n",
    "# print(tfidf_vectorizer.get_feature_names_out())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "239eed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('All_Beauty_5.json.gz')\n",
    "\n",
    "df = df.sort_values(by=['reviewerID', 'asin', 'unixReviewTime'])\n",
    "cleaned_dataset = df.dropna(subset=['overall']).drop_duplicates(subset=['reviewerID', 'asin'], keep = 'last').reset_index(drop=True)\n",
    "# print(len(cleaned_dataset))\n",
    "# cleaned_dataset.head()\n",
    "cleaned_dataset = cleaned_dataset.sort_values(by=['reviewerID', 'unixReviewTime']).reset_index(drop=True)\n",
    "# extracting the latest (in time) positively rated item (rating  ≥4 ) by each user. \n",
    "test_data_pre = cleaned_dataset[cleaned_dataset.overall >= 4.0].drop_duplicates(subset=['reviewerID'], keep='last')\n",
    "# generate training data\n",
    "training_data = cleaned_dataset.drop(test_data_pre.index)\n",
    "\n",
    "# Remove users that do not appear in the training set.\n",
    "user_in_training = test_data_pre['reviewerID'].isin(training_data['reviewerID'])\n",
    "test_data = test_data_pre[user_in_training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2500a974",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lambda bools: True if bools == False else False\n",
    "item_in = clean_dataset_item['asin'][clean_dataset_item['asin'].isin(training_data[training_data['reviewerID']=='A39WWMBA0299ZF']['asin'])]\n",
    "item_not_in = clean_dataset_item['asin'][[x(bools) for bools in clean_dataset_item['asin'].isin(training_data[training_data['reviewerID']=='A39WWMBA0299ZF']['asin'])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bc668dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pick_item_from_tfidf(tfidf_vectorizer:TfidfVectorizer, item_list:list, data_set:list):\n",
    "    vector_np = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names_out()).to_numpy()\n",
    "    vector_list = []\n",
    "    for i, item in enumerate(data_set):\n",
    "        if item in item_list:\n",
    "            vector_list.append(vector_np[i,:])\n",
    "    return vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_in = pick_item_from_tfidf(tfidf_vectorizer, list(item_in), list(clean_dataset_item['asin']))\n",
    "vector_not_in = pick_item_from_tfidf(tfidf_vectorizer, list(item_not_in), list(clean_dataset_item['asin']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2692e66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B019FWRG3C\n",
      "B00W259T7G\n",
      "B00006L9LC\n",
      "B002GP80EU\n",
      "B019809F9Y\n",
      "[[0.35285711]\n",
      " [0.16561569]\n",
      " [0.14454085]\n",
      " [0.12955529]\n",
      " [0.08856782]]\n"
     ]
    }
   ],
   "source": [
    "mean_vector = [np.mean(vector_in, axis=0)]\n",
    "cos_sim = cosine_similarity(vector_not_in,mean_vector)\n",
    "for i in np.argsort([-float(i) for i in cos_sim],)[:5]:\n",
    "    print(list(item_not_in)[i]) \n",
    "print(cos_sim[np.argsort([-float(i) for i in cos_sim])[:5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f2bbe",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7805a3",
   "metadata": {},
   "source": [
    "Compute the systems’ hit rate based on the top-5, top-10 and top-20 recommendations, averaged over the total number of users. Remember that, as we are evaluating the system, you should compute the hit rate over the test set. How well/bad does this Content-based approach perform compared to the Collaborative Filtering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7455795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1d74d43",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Repeat Exercise 1 and 2, this time representing the products and users in a word2vec vector space. You may use the gensim library and download the 300-dimension embeddings from Google. Source: https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "word2vec_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
