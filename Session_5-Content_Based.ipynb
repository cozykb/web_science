{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6011ca",
   "metadata": {},
   "source": [
    "# Content-based recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a455b8a",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Based on the TF-IDF vectors obtained in the Exercise 2 from Session 4, represent each user in the same vector space. Amongst other feasible solutions, you can represent a user (user profile) by computing the weighted mean of the items vectors. Compute the cosine similarity for user 'A39WWMBA0299ZF' and all products in the training set not rated by the user. What are the top-5 recommended items for user 'A39WWMBA0299ZF'? Print out the top-5 items and their similarity score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be516e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lwk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/lwk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "403ff409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load TRAIN and TEST sets \n",
    "test_data = pickle.load( open( \"test.pkl\", \"rb\" ) )\n",
    "train_data = pickle.load( open( \"training.pkl\", \"rb\" ) )\n",
    "\n",
    "# Load the METADATA (ITEMS)\n",
    "df = pd.read_json('meta_All_Beauty.json', lines=True)\n",
    "\n",
    "# Discard duplicates\n",
    "df = df.sort_values(by=['asin'])\n",
    "clean_dataset_item = df.drop_duplicates(subset=['asin'], keep = 'last').reset_index(drop=True)\n",
    "\n",
    "# Discard items that weren't rated by our subset of users\n",
    "item_in_subset = list(test_data.loc[:,'asin'])+list(train_data.loc[:,'asin'])\n",
    "# print(list(item_in_subset))\n",
    "clean_dataset_item = clean_dataset_item[clean_dataset_item['asin'].isin(item_in_subset)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'ui_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lwk/Documents/web_science/web_science/Session_5-Content_Based.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lwk/Documents/web_science/web_science/Session_5-Content_Based.ipynb#ch0000004?line=19'>20</a>\u001b[0m df_2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_json(\u001b[39m'\u001b[39m\u001b[39mmeta_All_Beauty.json\u001b[39m\u001b[39m'\u001b[39m, lines\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lwk/Documents/web_science/web_science/Session_5-Content_Based.ipynb#ch0000004?line=20'>21</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msummary\u001b[39;00m \u001b[39mimport\u001b[39;00m Contentbased_recommendation\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lwk/Documents/web_science/web_science/Session_5-Content_Based.ipynb#ch0000004?line=22'>23</a>\u001b[0m preprocess \u001b[39m=\u001b[39m Contentbased_recommendation(ui_df \u001b[39m=\u001b[39;49m df_1, item_df\u001b[39m=\u001b[39;49mdf_2)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'ui_df'"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def getDF(path):\n",
    "  def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "      yield json.loads(l)\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df_1 = getDF('All_Beauty_5.json.gz')\n",
    "df_2 = pd.read_json('meta_All_Beauty.json', lines=True)\n",
    "from summary import Contentbased_recommendation\n",
    "\n",
    "preprocess = Contentbased_recommendation(ui_df = df_1, item_df=df_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e080b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1039\n",
      "1002\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "len_words = 0\n",
    "len_filter_words = 0\n",
    "title_list = []\n",
    "temp_list = []\n",
    "for title in preprocess.clean_item_data['title']:\n",
    "    # print(title)\n",
    "    word_list = [word for word in word_tokenize(title)]\n",
    "    temp_list += word_list\n",
    "    # temp_list.append(word_list)\n",
    "len_words = len(temp_list)\n",
    "# temp_list_ = []\n",
    "# print(temp_list.count('3.5'))\n",
    "for title in list(preprocess.clean_item_data['title']):\n",
    "    filter_list = [porter_stemmer.stem(word) for word in word_tokenize(title) if word not in stopwords.words(\"english\")]\n",
    "    len_filter_words += len(filter_list)\n",
    "    # temp_list_ += filter_list\n",
    "    title_list.append(TreebankWordDetokenizer().detokenize(filter_list))\n",
    "    # title_list.append(\" \".join(filter))\n",
    "print(len_words)\n",
    "print(len_filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  05   10  100  1090  ...  yardley  you  youth       zum\n",
      "asin                                  ...                               \n",
      "B0000530HU  0.000000  0.0  0.0   0.0  ...      0.0  0.0    0.0  0.000000\n",
      "B00006L9LC  0.000000  0.0  0.0   0.0  ...      0.0  0.0    0.0  0.000000\n",
      "B00021DJ32  0.000000  0.0  0.0   0.0  ...      0.0  0.0    0.0  0.000000\n",
      "B0002JHI1I  0.000000  0.0  0.0   0.0  ...      0.0  0.0    0.0  0.000000\n",
      "B0006O10P4  0.000000  0.0  0.0   0.0  ...      0.0  0.0    0.0  0.782655\n",
      "...              ...  ...  ...   ...  ...      ...  ...    ...       ...\n",
      "B019LAI4HU  0.000000  0.0  0.0   0.0  ...      0.0  0.0    0.0  0.000000\n",
      "B019V2KYZS  0.434159  0.0  0.0   0.0  ...      0.0  0.0    0.0  0.000000\n",
      "B01BNEYGQU  0.000000  0.0  0.0   0.0  ...      0.0  0.0    0.0  0.000000\n",
      "B01DKQAXC0  0.000000  0.0  0.0   0.0  ...      0.0  0.0    0.0  0.000000\n",
      "B01E7UKR38  0.000000  0.0  0.0   0.0  ...      0.0  0.0    0.0  0.000000\n",
      "\n",
      "[84 rows x 449 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# <YOUR CODE HERE>\n",
    "X = tfidf_vectorizer.fit_transform(title_list)\n",
    "# print(tfidf_vectorizer.get_feature_names_out())\n",
    "print(pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index=preprocess.clean_item_data['asin']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c46275",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index=preprocess.clean_item_data['asin'])\n",
    "for line in test.you:\n",
    "    print( line )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "239eed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('All_Beauty_5.json.gz')\n",
    "\n",
    "df = df.sort_values(by=['reviewerID', 'asin', 'unixReviewTime'])\n",
    "cleaned_dataset = df.dropna(subset=['overall']).drop_duplicates(subset=['reviewerID', 'asin'], keep = 'last').reset_index(drop=True)\n",
    "# print(len(cleaned_dataset))\n",
    "# cleaned_dataset.head()\n",
    "cleaned_dataset = cleaned_dataset.sort_values(by=['reviewerID', 'unixReviewTime']).reset_index(drop=True)\n",
    "# extracting the latest (in time) positively rated item (rating  â‰¥4 ) by each user. \n",
    "test_data_pre = cleaned_dataset[cleaned_dataset.overall >= 4.0].drop_duplicates(subset=['reviewerID'], keep='last')\n",
    "# generate training data\n",
    "training_data = cleaned_dataset.drop(test_data_pre.index)\n",
    "\n",
    "# Remove users that do not appear in the training set.\n",
    "user_in_training = test_data_pre['reviewerID'].isin(training_data['reviewerID'])\n",
    "test_data = test_data_pre[user_in_training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2500a974",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lambda bools: True if bools == False else False\n",
    "item_in = preprocess.clean_item_data['asin'][preprocess.clean_item_data['asin'].isin(preprocess.training_data[preprocess.training_data['reviewerID']=='A39WWMBA0299ZF']['asin'])]\n",
    "item_not_in = preprocess.clean_item_data['asin'][[x(bools) for bools in preprocess.clean_item_data['asin'].isin(preprocess.training_data[preprocess.training_data['reviewerID']=='A39WWMBA0299ZF']['asin'])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc668dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pick_item_from_tfidf(tfidf_vectorizer:TfidfVectorizer, item_list:list, data_set:list):\n",
    "    vector_np = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    vector_list = []\n",
    "    for i, item in enumerate(data_set):\n",
    "        if item in item_list:\n",
    "            vector_list.append(vector_np[i,:])\n",
    "    return vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "get_feature_names_out not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/lwk/Documents/web_science/web_science/Session_5-Content_Based.ipynb Cell 11'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lwk/Documents/web_science/web_science/Session_5-Content_Based.ipynb#ch0000009?line=0'>1</a>\u001b[0m vector_in \u001b[39m=\u001b[39m pick_item_from_tfidf(X, \u001b[39mlist\u001b[39;49m(item_in), \u001b[39mlist\u001b[39;49m(preprocess\u001b[39m.\u001b[39;49mclean_item_data[\u001b[39m'\u001b[39;49m\u001b[39masin\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lwk/Documents/web_science/web_science/Session_5-Content_Based.ipynb#ch0000009?line=1'>2</a>\u001b[0m vector_not_in \u001b[39m=\u001b[39m pick_item_from_tfidf(X, \u001b[39mlist\u001b[39m(item_not_in), \u001b[39mlist\u001b[39m(preprocess\u001b[39m.\u001b[39mclean_item_data[\u001b[39m'\u001b[39m\u001b[39masin\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "\u001b[1;32m/Users/lwk/Documents/web_science/web_science/Session_5-Content_Based.ipynb Cell 10'\u001b[0m in \u001b[0;36mpick_item_from_tfidf\u001b[0;34m(tfidf_vectorizer, item_list, data_set)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lwk/Documents/web_science/web_science/Session_5-Content_Based.ipynb#ch0000008?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpick_item_from_tfidf\u001b[39m(tfidf_vectorizer:TfidfVectorizer, item_list:\u001b[39mlist\u001b[39m, data_set:\u001b[39mlist\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lwk/Documents/web_science/web_science/Session_5-Content_Based.ipynb#ch0000008?line=1'>2</a>\u001b[0m     vector_np \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(X\u001b[39m.\u001b[39mtoarray(), columns\u001b[39m=\u001b[39mtfidf_vectorizer\u001b[39m.\u001b[39;49mget_feature_names_out())\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lwk/Documents/web_science/web_science/Session_5-Content_Based.ipynb#ch0000008?line=2'>3</a>\u001b[0m     vector_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lwk/Documents/web_science/web_science/Session_5-Content_Based.ipynb#ch0000008?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, item \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_set):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/sparse/base.py:687\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/base.py?line=684'>685</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetnnz()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/base.py?line=685'>686</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/base.py?line=686'>687</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(attr \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: get_feature_names_out not found"
     ]
    }
   ],
   "source": [
    "vector_in = pick_item_from_tfidf(tfidf_vectorizer, list(item_in), list(clean_dataset_item['asin']))\n",
    "vector_not_in = pick_item_from_tfidf(tfidf_vectorizer, list(item_not_in), list(clean_dataset_item['asin']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2692e66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B019FWRG3C\n",
      "B00W259T7G\n",
      "B00006L9LC\n",
      "B002GP80EU\n",
      "B019809F9Y\n",
      "[[0.35285711]\n",
      " [0.16561569]\n",
      " [0.14454085]\n",
      " [0.12955529]\n",
      " [0.08856782]]\n"
     ]
    }
   ],
   "source": [
    "mean_vector = [np.mean(vector_in, axis=0)]\n",
    "cos_sim = cosine_similarity(vector_not_in,mean_vector)\n",
    "for i in np.argsort([-float(i) for i in cos_sim],)[:5]:\n",
    "    print(list(item_not_in)[i]) \n",
    "print(cos_sim[np.argsort([-float(i) for i in cos_sim])[:5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f2bbe",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7805a3",
   "metadata": {},
   "source": [
    "Compute the systemsâ€™ hit rate based on the top-5, top-10 and top-20 recommendations, averaged over the total number of users. Remember that, as we are evaluating the system, you should compute the hit rate over the test set. How well/bad does this Content-based approach perform compared to the Collaborative Filtering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7455795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def hr_k_user(filted_pred_list, user_id, cut_off, relevant_matrix:pd.DataFrame):\n",
    "    for i in range(cut_off):\n",
    "        item_id = filted_pred_list[i][0]\n",
    "        try:\n",
    "            if relevant_matrix.loc[user_id, item_id] == 1:\n",
    "                return 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return 0\n",
    "\n",
    "def mean_k(pred_list, cut_off, function, relevant_matrix:pd.DataFrame):\n",
    "    user_list = []\n",
    "    user_list = [item.uid for item in pred_list if item.uid not in user_list]\n",
    "    num_users = len(relevant_matrix.index)\n",
    "    summation = []\n",
    "    user_item_rating = defaultdict(list)\n",
    "    for pred in pred_list:\n",
    "        user_item_rating[pred.uid].append((pred.iid, pred.est))\n",
    "    for user_id, filted_pred_list in user_item_rating.items():\n",
    "        filted_pred_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        summation.append(function(filted_pred_list, user_id, cut_off, relevant_matrix))\n",
    "    return sum(summation)/float(num_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d74d43",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Repeat Exercise 1 and 2, this time representing the products and users in a word2vec vector space. You may use the gensim library and download the 300-dimension embeddings from Google. Source: https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "word2vec_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
