{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uZ3Rb6JRfF3"
   },
   "source": [
    "# Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s5yo2Dqjqef"
   },
   "source": [
    "Please, note that this notebook is intended to be run in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dkMH-1WClvTd"
   },
   "outputs": [],
   "source": [
    "# Mount drive and define path to the data folder (from your Google Drive)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# datapath = 'drive/MyDrive/data/amazon_reviews/All_Beauty/'\n",
    "train_file = 'training.pkl'\n",
    "test_file = 'test.pkl'\n",
    "meta_file = 'meta_All_Beauty.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIYb2Pxi6_Ie"
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "Load the [metadata file](https://nijianmo.github.io/amazon/index.html) and discard any item that was not rated by our subset of users (nor in training or test sets). Apply preprocessing (stemming and stopwords removal) to clean up the text from the \"title\". Report the vocabulary size before and after the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CKbUQSlc65kO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load TRAIN and TEST sets \n",
    "test_data = pickle.load( open( \"test.pkl\", \"rb\" ) )\n",
    "train_data = pickle.load( open( \"training.pkl\", \"rb\" ) )\n",
    "\n",
    "# Load the METADATA (ITEMS)\n",
    "df = pd.read_json('meta_All_Beauty.json', lines=True)\n",
    "\n",
    "# Discard duplicates\n",
    "df = df.sort_values(by=['asin'])\n",
    "cleaned_dataset = df.drop_duplicates(subset=['asin'], keep = 'last').reset_index(drop=True)\n",
    "\n",
    "# Discard items that weren't rated by our subset of users\n",
    "item_in_subset = list(test_data.loc[:,'asin'])+list(train_data.loc[:,'asin'])\n",
    "# print(list(item_in_subset))\n",
    "cleaned_dataset = cleaned_dataset.loc[cleaned_dataset['asin'].isin(item_in_subset)]\n",
    "cleaned_dataset = cleaned_dataset.drop_duplicates(subset=['asin'], keep = 'last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lwk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/lwk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Rd0RbH-k9y1H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545\n",
      "712\n",
      "402\n"
     ]
    }
   ],
   "source": [
    "def isfloat(input:str):\n",
    "    try:\n",
    "        float(input)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# <YOUR CODE HERE>\n",
    "porter_stemmer = PorterStemmer()\n",
    "len_words = 0\n",
    "len_filter_words = 0\n",
    "title_list = []\n",
    "temp_list = []\n",
    "for title in cleaned_dataset['title']:\n",
    "    # print(title)\n",
    "    word_list = [word for word in word_tokenize(title)]\n",
    "    temp_list += word_list\n",
    "    # temp_list.append(word_list)\n",
    "len_words = len(set(temp_list))\n",
    "temp_list_ = []\n",
    "# print(temp_list.count('3.5'))\n",
    "for title in cleaned_dataset['title']:\n",
    "    filter_list = [porter_stemmer.stem(word.lower()) for word in word_tokenize(title) if word not in stopwords.words(\"english\") and word.isalpha()]\n",
    "    len_filter_words += len(filter_list)\n",
    "    temp_list_ += filter_list\n",
    "    title_list.append(TreebankWordDetokenizer().detokenize(filter_list))\n",
    "    # title_list.append(\" \".join(filter))\n",
    "print(len_words)\n",
    "print(len_filter_words)\n",
    "print(len(set(temp_list_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Blr1jgoHLbFU"
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "Representation in vector spaces.\n",
    "\n",
    "## 2.1\n",
    "\n",
    "Represent all the products from Exercise 1 in a TF-IDF space. Interpret the meaning of the TF-IDF matrix dimensions.\n",
    "\n",
    "Tip: You may use the library [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "vDndolvDLznV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 398)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# <YOUR CODE HERE>\n",
    "X = tfidf_vectorizer.fit_transform(title_list)\n",
    "# print(tfidf_vectorizer.get_feature_names_out())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gnTVM0EV_2d"
   },
   "source": [
    "## 2.2\n",
    "\n",
    "Compute and the cosine similarity between products with asin 'B000FI4S1E', 'B000LIBUBY' and 'B000W0C07Y'. Take a look at their features to see whether results make sense with their characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(np_arrays):\n",
    "    output = np.zeros((len(np_arrays),len(np_arrays)))\n",
    "    for i,np_array_i  in enumerate(np_arrays):\n",
    "        for j,np_array_j  in enumerate(np_arrays):\n",
    "            output[i,j] = (np_array_i.T.dot(np_array_j))/(np.linalg.norm(np_array_i)*np.linalg.norm(np_array_j))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zO_OHMY8PWbO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "10\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vector_np = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names()).to_numpy()\n",
    "vector_list = []\n",
    "item_list = ['B000FI4S1E', 'B000LIBUBY', 'B000W0C07Y']\n",
    "for i, item in enumerate(cleaned_dataset['asin']):\n",
    "    if item in item_list:\n",
    "        vector_list.append(vector_np[i,:])\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.03382796 0.02339948]\n",
      " [0.03382796 1.         0.40755955]\n",
      " [0.02339948 0.40755955 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "out = cos_sim(vector_list)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2\n",
      "0  1.000000  0.033828  0.023399\n",
      "1  0.033828  1.000000  0.407560\n",
      "2  0.023399  0.407560  1.000000\n"
     ]
    }
   ],
   "source": [
    "out = pd.DataFrame(cosine_similarity(vector_list))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0K8jRhWhZQWe"
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "Representation in vector spaces with contextual Word Embeddings.\n",
    "\n",
    "## 3.1.\n",
    "\n",
    "Represent all the products from Exercise 1 in a vector space using embeddings from a pre-trained BERT model. The final embedding of a product should be the average of the word embeddings from all the words in the 'title'. What is the vocabulary size of the model? What are the dimensions of the last hidden state?\n",
    "\n",
    "Tip: you may install the transformers library and use their pretrained [BERT model uncased](https://huggingface.co/bert-base-uncased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hHIjJ-LbTB3H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOAD TRANSFORMER\n",
    "\"\"\"\n",
    "If you plan on using a pretrained model, it’s important to use the associated \n",
    "pretrained tokenizer: it will split the text you give it in tokens the same way\n",
    "for the pretraining corpus, and it will use the same correspondence\n",
    "token to index (that we usually call a vocab) as during pretraining.\n",
    "\"\"\"\n",
    "\n",
    "# % pip install transformers\n",
    "import torch\n",
    "import transformers\n",
    "assert transformers.__version__ > '4.0.0'\n",
    "\n",
    "from transformers import BertModel, BertTokenizerFast, BertConfig\n",
    "\n",
    "# set-up environment\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "modelname = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(modelname)\n",
    "model = BertModel.from_pretrained(modelname).to(DEVICE)\n",
    "\n",
    "# Print out the vocabulary size\n",
    "# <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(cleaned_dataset['title'])\n",
    "# encoded_input = tokenizer(text[:4], max_length=100,\n",
    "#                           add_special_tokens=True, truncation=True,\n",
    "#                           padding=True, return_tensors=\"pt\")\n",
    "encoded_input = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
    "output = model(**encoded_input)\n",
    "c = model(**encoded_input)\n",
    "#last_hidden_state, pooler_output = output[0], output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([84, 52, 768])\n"
     ]
    }
   ],
   "source": [
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(BertConfig().vocab_size)\n",
    "print(BertConfig().hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "q_Symyv5U07x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNote that the control token [CLS] has been added \\nat the beginning of each sentence, and [SEP] at the end. \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REPRESENT PRODUCTS IN A VECTOR SPACE\n",
    "\n",
    "\n",
    "def batch_encoding(sentences):\n",
    "    # Since we're using padding, we need to provide the attention masks to our\n",
    "    # model. Otherwise it doesn't know which tokens it should not attend to. \n",
    "    inputs = tokenizer(sentences,padding=True,return_tensors=\"pt\",return_attention_mask=True)\n",
    "    # print(inputs) # Look at the padding and attention_mask\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    last_hidden_states = outputs[0]\n",
    "\n",
    "    return inputs, last_hidden_states\n",
    "  \n",
    "encoded_inputs, title_last_hidden_states = batch_encoding(list(cleaned_dataset['title']))\n",
    "\n",
    "\"\"\"\n",
    "Note that the control token [CLS] has been added \n",
    "at the beginning of each sentence, and [SEP] at the end. \n",
    "\"\"\"\n",
    "\n",
    "# Now, let's mask out the padding tokens and compute the embedding vector of each product\n",
    "\n",
    "# <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwRBr2HP0Zdt"
   },
   "source": [
    "## 3.2.\n",
    "\n",
    "Compute and the cosine similarity between products with asin 'B000FI4S1E', 'B000LIBUBY' and 'B000W0C07Y'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OaHxSLHqItNs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "item_list = ['B000FI4S1E', 'B000LIBUBY', 'B000W0C07Y']\n",
    "mask_list = []\n",
    "vector_list = []\n",
    "return_list = []\n",
    "for i, item in enumerate(cleaned_dataset['asin']):\n",
    "    if item in item_list:\n",
    "        mask_list.append(encoded_inputs[\"attention_mask\"][i])\n",
    "        vector_list.append(title_last_hidden_states[i])\n",
    "        return_list.append(encoded_inputs[\"attention_mask\"][i].detach().numpy().dot(\n",
    "            title_last_hidden_states[i].detach().numpy())/np.sum(encoded_inputs[\"attention_mask\"][i].detach().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.73359339 0.65935059]\n",
      " [0.73359339 1.         0.74751203]\n",
      " [0.65935059 0.74751203 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(return_list))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Session_4-Text-Representation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
